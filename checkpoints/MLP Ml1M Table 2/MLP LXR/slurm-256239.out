wandb: Currently logged in as: zwsdz123 (innsbruck). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/amir.reza/counterfactual/CFX-REC/wandb/run-20240724_151249-51105ehw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-glitter-102
wandb: â­ï¸ View project at https://wandb.ai/innsbruck/my-awesome-project
wandb: ğŸš€ View run at https://wandb.ai/innsbruck/my-awesome-project/runs/51105ehw
wandb: - 0.005 MB of 0.008 MB uploadedwandb: \ 0.005 MB of 0.008 MB uploadedwandb: | 0.008 MB of 0.008 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  acc â–‚â–â–‡â–†â–ˆâ–‡â–ˆâ–‡
wandb: loss â–ˆâ–ˆâ–…â–ƒâ–‚â–â–‚â–
wandb: 
wandb: Run summary:
wandb:  acc 0.88398
wandb: loss 0.06963
wandb: 
wandb: ğŸš€ View run worldly-glitter-102 at: https://wandb.ai/innsbruck/my-awesome-project/runs/51105ehw
wandb: â­ï¸ View project at: https://wandb.ai/innsbruck/my-awesome-project
wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_151249-51105ehw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/amir.reza/counterfactual/CFX-REC/wandb/run-20240724_151257-02gc1g88
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_0
wandb: â­ï¸ View project at https://wandb.ai/innsbruck/ML1M_MLP_LXR_training
wandb: ğŸš€ View run at https://wandb.ai/innsbruck/ML1M_MLP_LXR_training/runs/02gc1g88
method MLP @ 10
model is MLP_ML1M_2_512.pt
======================== new run ========================
Finished epoch 0 with run_pos_at_k 0.645909090909091 and run_neg_at_k 0.894090909090909
Train loss = 28415.7421875
saving the checkpoint in epch 1 with filename: 2LXR_ML1M_MLP_0_1_32_46.54931757000438_19.14191145091549.pt
Finished epoch 1 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.895
Train loss = 2899.77197265625
saving the checkpoint in epch 2 with filename: 2LXR_ML1M_MLP_0_2_32_46.54931757000438_19.14191145091549.pt
Finished epoch 2 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8931818181818181
Train loss = 563.2792358398438
saving the checkpoint in epch 3 with filename: 2LXR_ML1M_MLP_0_3_32_46.54931757000438_19.14191145091549.pt
Finished epoch 3 with run_pos_at_k 0.6313636363636363 and run_neg_at_k 0.8927272727272727
Train loss = -111.58076477050781
saving the checkpoint in epch 4 with filename: 2LXR_ML1M_MLP_0_4_32_46.54931757000438_19.14191145091549.pt
Finished epoch 4 with run_pos_at_k 0.6327272727272727 and run_neg_at_k 0.8922727272727273
Train loss = -441.28643798828125
saving the checkpoint in epch 5 with filename: 2LXR_ML1M_MLP_0_5_32_46.54931757000438_19.14191145091549.pt
Finished epoch 5 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8918181818181818
Train loss = -635.097412109375
saving the checkpoint in epch 6 with filename: 2LXR_ML1M_MLP_0_6_32_46.54931757000438_19.14191145091549.pt
Finished epoch 6 with run_pos_at_k 0.6322727272727273 and run_neg_at_k 0.8931818181818181
Train loss = -764.6737060546875
saving the checkpoint in epch 7 with filename: 2LXR_ML1M_MLP_0_7_32_46.54931757000438_19.14191145091549.pt
Finished epoch 7 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8922727272727273
Train loss = -854.1743774414062
saving the checkpoint in epch 8 with filename: 2LXR_ML1M_MLP_0_8_32_46.54931757000438_19.14191145091549.pt
Finished epoch 8 with run_pos_at_k 0.6327272727272727 and run_neg_at_k 0.8931818181818181
Train loss = -919.9302978515625
saving the checkpoint in epch 9 with filename: 2LXR_ML1M_MLP_0_9_32_46.54931757000438_19.14191145091549.pt
Finished epoch 9 with run_pos_at_k 0.6327272727272727 and run_neg_at_k 0.8918181818181818
Train loss = -969.396240234375
saving the checkpoint in epch 10 with filename: 2LXR_ML1M_MLP_0_10_32_46.54931757000438_19.14191145091549.pt
Finished epoch 10 with run_pos_at_k 0.6327272727272727 and run_neg_at_k 0.8931818181818181
Train loss = -1006.5881958007812
saving the checkpoint in epch 11 with filename: 2LXR_ML1M_MLP_0_11_32_46.54931757000438_19.14191145091549.pt
Finished epoch 11 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8931818181818181
Train loss = -1036.804443359375
saving the checkpoint in epch 12 with filename: 2LXR_ML1M_MLP_0_12_32_46.54931757000438_19.14191145091549.pt
Finished epoch 12 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8931818181818181
Train loss = -1062.916259765625
saving the checkpoint in epch 13 with filename: 2LXR_ML1M_MLP_0_13_32_46.54931757000438_19.14191145091549.pt
Finished epoch 13 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8927272727272727
Train loss = -1087.3038330078125
saving the checkpoint in epch 14 with filename: 2LXR_ML1M_MLP_0_14_32_46.54931757000438_19.14191145091549.pt
Finished epoch 14 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8927272727272727
Train loss = -1106.8109130859375
saving the checkpoint in epch 15 with filename: 2LXR_ML1M_MLP_0_15_32_46.54931757000438_19.14191145091549.pt
Finished epoch 15 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8922727272727273
Train loss = -1130.45263671875
saving the checkpoint in epch 16 with filename: 2LXR_ML1M_MLP_0_16_32_46.54931757000438_19.14191145091549.pt
Finished epoch 16 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8931818181818181
Train loss = -1148.3695068359375
saving the checkpoint in epch 17 with filename: 2LXR_ML1M_MLP_0_17_32_46.54931757000438_19.14191145091549.pt
Finished epoch 17 with run_pos_at_k 0.6322727272727273 and run_neg_at_k 0.8931818181818181
Train loss = -1158.82080078125
saving the checkpoint in epch 18 with filename: 2LXR_ML1M_MLP_0_18_32_46.54931757000438_19.14191145091549.pt
Finished epoch 18 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8927272727272727
Train loss = -1169.1287841796875
saving the checkpoint in epch 19 with filename: 2LXR_ML1M_MLP_0_19_32_46.54931757000438_19.14191145091549.pt
Finished epoch 19 with run_pos_at_k 0.6327272727272727 and run_neg_at_k 0.8927272727272727
Train loss = -1179.6417236328125
saving the checkpoint in epch 20 with filename: 2LXR_ML1M_MLP_0_20_32_46.54931757000438_19.14191145091549.pt
Finished epoch 20 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8927272727272727
Train loss = -1187.4420166015625
saving the checkpoint in epch 21 with filename: 2LXR_ML1M_MLP_0_21_32_46.54931757000438_19.14191145091549.pt
Finished epoch 21 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8927272727272727
Train loss = -1196.6636962890625
saving the checkpoint in epch 22 with filename: 2LXR_ML1M_MLP_0_22_32_46.54931757000438_19.14191145091549.pt
Finished epoch 22 with run_pos_at_k 0.634090909090909 and run_neg_at_k 0.8927272727272727
Train loss = -1201.4276123046875
saving the checkpoint in epch 23 with filename: 2LXR_ML1M_MLP_0_23_32_46.54931757000438_19.14191145091549.pt
Finished epoch 23 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8927272727272727
Train loss = -1208.6273193359375
saving the checkpoint in epch 24 with filename: 2LXR_ML1M_MLP_0_24_32_46.54931757000438_19.14191145091549.pt
Finished epoch 24 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8927272727272727
Train loss = -1212.5400390625
saving the checkpoint in epch 25 with filename: 2LXR_ML1M_MLP_0_25_32_46.54931757000438_19.14191145091549.pt
Finished epoch 25 with run_pos_at_k 0.6327272727272727 and run_neg_at_k 0.8927272727272727
Train loss = -1216.32373046875
saving the checkpoint in epch 26 with filename: 2LXR_ML1M_MLP_0_26_32_46.54931757000438_19.14191145091549.pt
Finished epoch 26 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8927272727272727
Train loss = -1220.411865234375
saving the checkpoint in epch 27 with filename: 2LXR_ML1M_MLP_0_27_32_46.54931757000438_19.14191145091549.pt
Finished epoch 27 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8927272727272727
Train loss = -1223.0252685546875
saving the checkpoint in epch 28 with filename: 2LXR_ML1M_MLP_0_28_32_46.54931757000438_19.14191145091549.pt
Finished epoch 28 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8927272727272727
Train loss = -1226.8482666015625
saving the checkpoint in epch 29 with filename: 2LXR_ML1M_MLP_0_29_32_46.54931757000438_19.14191145091549.pt
Finished epoch 29 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8927272727272727
Train loss = -1228.6182861328125
saving the checkpoint in epch 30 with filename: 2LXR_ML1M_MLP_0_30_32_46.54931757000438_19.14191145091549.pt
Finished epoch 30 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8927272727272727
Train loss = -1233.93017578125
saving the checkpoint in epch 31 with filename: 2LXR_ML1M_MLP_0_31_32_46.54931757000438_19.14191145091549.pt
Finished epoch 31 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8927272727272727
Train loss = -1235.963623046875
saving the checkpoint in epch 32 with filename: 2LXR_ML1M_MLP_0_32_32_46.54931757000438_19.14191145091549.pt
Finished epoch 32 with run_pos_at_k 0.634090909090909 and run_neg_at_k 0.8927272727272727
Train loss = -1239.5626220703125
saving the checkpoint in epch 33 with filename: 2LXR_ML1M_MLP_0_33_32_46.54931757000438_19.14191145091549.pt
Finished epoch 33 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8927272727272727
Train loss = -1241.1292724609375
saving the checkpoint in epch 34 with filename: 2LXR_ML1M_MLP_0_34_32_46.54931757000438_19.14191145091549.pt
Finished epoch 34 with run_pos_at_k 0.6331818181818182 and run_neg_at_k 0.8927272727272727
Train loss = -1242.4970703125
saving the checkpoint in epch 35 with filename: 2LXR_ML1M_MLP_0_35_32_46.54931757000438_19.14191145091549.pt
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.006 MB uploadedwandb: | 0.005 MB of 0.006 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train/epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:    train/l1_loss â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   train/neg_loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train/pos_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: train/train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     val/neg_at_k â–†â–ˆâ–„â–ƒâ–‚â–â–„â–‚â–„â–â–„â–„â–„â–ƒâ–ƒâ–‚â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒ
wandb:     val/pos_at_k â–ˆâ–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      train/epoch 39
wandb:    train/l1_loss 3845.53687
wandb:   train/neg_loss -3690.89038
wandb:   train/pos_loss 1408.20178
wandb: train/train_loss -1254.33044
wandb:     val/neg_at_k 0.89273
wandb:     val/pos_at_k 0.63455
wandb: 
wandb: ğŸš€ View run trial_0 at: https://wandb.ai/innsbruck/ML1M_MLP_LXR_training/runs/02gc1g88
wandb: â­ï¸ View project at: https://wandb.ai/innsbruck/ML1M_MLP_LXR_training
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_151257-02gc1g88/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Finished epoch 35 with run_pos_at_k 0.6336363636363637 and run_neg_at_k 0.8927272727272727
Train loss = -1244.1441650390625
saving the checkpoint in epch 36 with filename: 2LXR_ML1M_MLP_0_36_32_46.54931757000438_19.14191145091549.pt
Finished epoch 36 with run_pos_at_k 0.634090909090909 and run_neg_at_k 0.8927272727272727
Train loss = -1246.284912109375
saving the checkpoint in epch 37 with filename: 2LXR_ML1M_MLP_0_37_32_46.54931757000438_19.14191145091549.pt
Finished epoch 37 with run_pos_at_k 0.634090909090909 and run_neg_at_k 0.8927272727272727
Train loss = -1249.527587890625
saving the checkpoint in epch 38 with filename: 2LXR_ML1M_MLP_0_38_32_46.54931757000438_19.14191145091549.pt
Finished epoch 38 with run_pos_at_k 0.6345454545454545 and run_neg_at_k 0.8931818181818181
Train loss = -1252.36083984375
saving the checkpoint in epch 39 with filename: 2LXR_ML1M_MLP_0_39_32_46.54931757000438_19.14191145091549.pt
Finished epoch 39 with run_pos_at_k 0.6345454545454545 and run_neg_at_k 0.8927272727272727
Train loss = -1254.3304443359375
Stop at trial with lambda_pos = 46.54931757000438, lambda_neg = 19.14191145091549, alpha_parameter = 1. Best results at epoch 3 with value 0.6313636363636363
Best hyperparameters: {'learning_rate': 0.0027828840567442103, 'alpha': 1, 'lambda_neg': 19.14191145091549, 'lambda_pos': 46.54931757000438, 'batch_size': 128, 'explainer_hidden_size': 32}
Best metric value: 0.6313636363636363
